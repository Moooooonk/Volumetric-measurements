{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volumetric Measurement Experiments\n",
    "## Based on Reference Papers\n",
    "\n",
    "**References:**\n",
    "- **박문기** - 체적 측정: Icosphere 해상도별 정밀도, Green's/Divergence Theorem, 비수밀 메쉬 Capping\n",
    "- **박도현 (DIMA 69차)** - 림프부종 체적 측정: Slice-based Volume, Truncated Cone, Bland-Altman, TEM/CV/ICC\n",
    "\n",
    "**Experiments:**\n",
    "1. Icosphere Resolution vs Volume Accuracy (Level 1-5)\n",
    "2. Green's Theorem 2D - Cross-Section Area\n",
    "3. Slice-Based Volume (1mm slices + Smoothing Splines + Green's Equation)\n",
    "4. Truncated Cone Approximation\n",
    "5. Planar Capping for Non-Watertight Meshes\n",
    "6. Mesh Resolution Convergence Study\n",
    "7. Bland-Altman Analysis & Statistical Metrics (TEM, CV, ICC)\n",
    "8. Comprehensive Comparison & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Environment Setup ---\nimport os\n\n# Detect environment\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    IN_COLAB = True\n    PROJECT_DIR = '/content/drive/MyDrive/Volumetric_measurements'\n    WORK_DIR = '/content/volumetric_exp'\n    os.makedirs(WORK_DIR, exist_ok=True)\n    os.chdir(WORK_DIR)\n    # Extract bunny data from Drive to local\n    import tarfile\n    src = os.path.join(PROJECT_DIR, 'bunny.tar.gz')\n    if not os.path.exists('bunny'):\n        print(f'Extracting {src} ...')\n        with tarfile.open(src, 'r:gz') as tar:\n            tar.extractall('.')\n        print('Done!')\n    else:\n        print('bunny/ already exists.')\nexcept ImportError:\n    IN_COLAB = False\n    # Local execution - use current script directory\n    WORK_DIR = os.path.dirname(os.path.abspath('volumetric_experiments.ipynb'))\n    if not WORK_DIR:\n        WORK_DIR = os.getcwd()\n    os.chdir(WORK_DIR)\n    PROJECT_DIR = WORK_DIR\n    print(f'Running locally (not Colab)')\n    # Extract if needed\n    if not os.path.exists('bunny') and os.path.exists('bunny.tar.gz'):\n        import tarfile\n        with tarfile.open('bunny.tar.gz', 'r:gz') as tar:\n            tar.extractall('.')\n        print('Extracted bunny.tar.gz')\n\nprint(f'Working directory: {os.getcwd()}')\nprint(f'Project directory: {PROJECT_DIR}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install packages (Colab only; local env should have these pre-installed)\nif IN_COLAB:\n    !pip install -q plotly trimesh rtree scikit-image tqdm shapely\nelse:\n    print('Local environment - skipping pip install')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, warnings\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import UnivariateSpline, splprep, splev\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json, shutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'NumPy: {np.__version__}, SciPy: {scipy.__version__}, Trimesh: {trimesh.__version__}')\n",
    "print('All libraries loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLY Parser & Data Loading ----\n",
    "def parse_ply(filepath):\n",
    "    \"\"\"Parse ASCII PLY -> vertices (N,D), faces (M,3), property names.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    n_verts = n_faces = 0\n",
    "    header_end = 0\n",
    "    props = []\n",
    "    in_vertex_elem = False\n",
    "    for i, line in enumerate(lines):\n",
    "        s = line.strip()\n",
    "        if s.startswith('element vertex'):\n",
    "            n_verts = int(s.split()[-1]); in_vertex_elem = True\n",
    "        elif s.startswith('element'):\n",
    "            if 'face' in s: n_faces = int(s.split()[-1])\n",
    "            in_vertex_elem = False\n",
    "        elif s.startswith('property') and in_vertex_elem:\n",
    "            props.append(s.split()[-1])\n",
    "        elif s == 'end_header':\n",
    "            header_end = i + 1; break\n",
    "    verts = np.array([[float(x) for x in lines[header_end + j].split()] for j in range(n_verts)])\n",
    "    faces = None\n",
    "    if n_faces > 0:\n",
    "        face_start = header_end + n_verts\n",
    "        faces = np.array([[int(x) for x in lines[face_start + j].split()[1:4]] for j in range(n_faces)])\n",
    "    return verts, faces, props\n",
    "\n",
    "# Load all bunny resolutions\n",
    "BASE = Path('bunny/reconstruction')\n",
    "mesh_files = {\n",
    "    'Full (35k)': 'bun_zipper.ply',\n",
    "    'Res2 (8k)': 'bun_zipper_res2.ply',\n",
    "    'Res3 (1.9k)': 'bun_zipper_res3.ply',\n",
    "    'Res4 (453)': 'bun_zipper_res4.ply',\n",
    "}\n",
    "meshes = {}\n",
    "for label, fname in mesh_files.items():\n",
    "    v, f, p = parse_ply(BASE / fname)\n",
    "    meshes[label] = {'vertices': v[:, :3], 'faces': f}\n",
    "    print(f'{label:16s} | {v.shape[0]:>6,} verts | {f.shape[0]:>6,} faces')\n",
    "\n",
    "vertices = meshes['Full (35k)']['vertices']\n",
    "faces = meshes['Full (35k)']['faces']\n",
    "vertices_light = meshes['Res2 (8k)']['vertices']\n",
    "faces_light = meshes['Res2 (8k)']['faces']\n",
    "print(f'\\nMain mesh: {len(vertices):,} verts, {len(faces):,} faces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Volume Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def volume_divergence_theorem(verts, faces):\n    \"\"\"Volume via Divergence Theorem.\n    \n    V = (1/3) ∮ r·dS = (1/3) Σ_faces [centroid · (cross/2)]\n      = (1/6) Σ centroid · cross\n    \n    where centroid = (v1+v2+v3)/3, cross = (v2-v1)×(v3-v1)\n    \n    Since centroid·cross = (v1+v2+v3)/3 · cross, we get:\n    V = (1/6) Σ (v1+v2+v3)/3 · cross = |Σ contributions| / 6\n    \"\"\"\n    v1, v2, v3 = verts[faces[:,0]], verts[faces[:,1]], verts[faces[:,2]]\n    cross = np.cross(v2 - v1, v3 - v1)\n    centroid = (v1 + v2 + v3) / 3.0\n    contributions = np.sum(centroid * cross, axis=1)\n    return abs(contributions.sum()) / 6.0\n\ndef volume_signed_tetrahedra(verts, faces):\n    \"\"\"V = |sum (1/6) * v1 . (v2 x v3)|\"\"\"\n    v1, v2, v3 = verts[faces[:,0]], verts[faces[:,1]], verts[faces[:,2]]\n    signed_vols = np.sum(v1 * np.cross(v2, v3), axis=1) / 6.0\n    return abs(signed_vols.sum())\n\ndef greens_theorem_2d(x, y):\n    \"\"\"Area of 2D closed polygon using Green's theorem: A = 0.5 * |sum(x_i*y_{i+1} - x_{i+1}*y_i)|\n    Also known as the Shoelace formula, derived from Green's theorem.\"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n    n = len(x)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += x[i] * y[j] - x[j] * y[i]\n    return abs(area) / 2.0\n\ndef greens_theorem_2d_vectorized(x, y):\n    \"\"\"Vectorized Green's theorem (Shoelace formula).\"\"\"\n    x, y = np.asarray(x), np.asarray(y)\n    return abs(np.sum(x * np.roll(y, -1) - np.roll(x, -1) * y)) / 2.0\n\nprint('Core volume functions defined.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Icosphere Resolution vs Volume Accuracy\n",
    "\n",
    "**Reference: 박문기 p.2**\n",
    "\n",
    "Generate icospheres (unit sphere approximations) at Level 1-5 subdivision.  \n",
    "Compare computed volume (Divergence Theorem) with analytical volume $V = \\frac{4}{3}\\pi r^3 = 4.18879...$\n",
    "\n",
    "| Level | Vertices | Faces | Expected Error |\n",
    "|-------|----------|-------|-----------|\n",
    "| 1     | 42       | 80    | ~12.65%   |\n",
    "| 2     | 162      | 320   | ~3.37%    |\n",
    "| 3     | 642      | 1280  | ~0.856%   |\n",
    "| 4     | 2562     | 5120  | ~0.215%   |\n",
    "| 5     | 10242    | 20480 | ~0.054%   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_icosphere(subdivisions=0, radius=1.0):\n",
    "    \"\"\"Create an icosphere by subdividing an icosahedron.\"\"\"\n",
    "    # Base icosahedron\n",
    "    t = (1.0 + np.sqrt(5.0)) / 2.0\n",
    "    verts = np.array([\n",
    "        [-1, t, 0], [1, t, 0], [-1, -t, 0], [1, -t, 0],\n",
    "        [0, -1, t], [0, 1, t], [0, -1, -t], [0, 1, -t],\n",
    "        [t, 0, -1], [t, 0, 1], [-t, 0, -1], [-t, 0, 1],\n",
    "    ], dtype=np.float64)\n",
    "    # Normalize to unit sphere\n",
    "    norms = np.linalg.norm(verts, axis=1, keepdims=True)\n",
    "    verts = verts / norms\n",
    "\n",
    "    faces = np.array([\n",
    "        [0,11,5],[0,5,1],[0,1,7],[0,7,10],[0,10,11],\n",
    "        [1,5,9],[5,11,4],[11,10,2],[10,7,6],[7,1,8],\n",
    "        [3,9,4],[3,4,2],[3,2,6],[3,6,8],[3,8,9],\n",
    "        [4,9,5],[2,4,11],[6,2,10],[8,6,7],[9,8,1],\n",
    "    ], dtype=np.int64)\n",
    "\n",
    "    # Subdivide\n",
    "    for _ in range(subdivisions):\n",
    "        edge_midpoints = {}\n",
    "        new_faces = []\n",
    "        n_verts = len(verts)\n",
    "\n",
    "        def get_midpoint(i, j):\n",
    "            key = (min(i, j), max(i, j))\n",
    "            if key in edge_midpoints:\n",
    "                return edge_midpoints[key]\n",
    "            mid = (verts[i] + verts[j]) / 2.0\n",
    "            mid = mid / np.linalg.norm(mid)  # Project to sphere\n",
    "            idx = len(verts_list)\n",
    "            verts_list.append(mid)\n",
    "            edge_midpoints[key] = idx\n",
    "            return idx\n",
    "\n",
    "        verts_list = list(verts)\n",
    "        for tri in faces:\n",
    "            a, b, c = tri\n",
    "            ab = get_midpoint(a, b)\n",
    "            bc = get_midpoint(b, c)\n",
    "            ca = get_midpoint(c, a)\n",
    "            new_faces.extend([[a, ab, ca], [b, bc, ab], [c, ca, bc], [ab, bc, ca]])\n",
    "\n",
    "        verts = np.array(verts_list)\n",
    "        faces = np.array(new_faces, dtype=np.int64)\n",
    "\n",
    "    verts = verts * radius\n",
    "    return verts, faces\n",
    "\n",
    "# Analytical volume of unit sphere\n",
    "V_analytical = (4.0 / 3.0) * np.pi  # = 4.188790...\n",
    "A_analytical = 4.0 * np.pi            # = 12.566371...\n",
    "\n",
    "print(f'Analytical sphere volume: {V_analytical:.6f}')\n",
    "print(f'Analytical sphere area  : {A_analytical:.6f}')\n",
    "print()\n",
    "\n",
    "ico_results = []\n",
    "for level in range(1, 6):\n",
    "    v, f = create_icosphere(subdivisions=level, radius=1.0)\n",
    "    vol_div = volume_divergence_theorem(v, f)\n",
    "    vol_tet = volume_signed_tetrahedra(v, f)\n",
    "    mesh_tm = trimesh.Trimesh(vertices=v, faces=f)\n",
    "    area = mesh_tm.area\n",
    "    err_vol = abs(vol_div - V_analytical) / V_analytical * 100\n",
    "    err_area = abs(area - A_analytical) / A_analytical * 100\n",
    "\n",
    "    ico_results.append({\n",
    "        'Level': level,\n",
    "        'Vertices': len(v),\n",
    "        'Faces': len(f),\n",
    "        'Volume (Div)': vol_div,\n",
    "        'Volume (Tet)': vol_tet,\n",
    "        'Surface Area': area,\n",
    "        'Vol Error (%)': err_vol,\n",
    "        'Area Error (%)': err_area,\n",
    "        'Watertight': mesh_tm.is_watertight,\n",
    "    })\n",
    "    print(f'Level {level}: {len(v):>6,} verts, {len(f):>6,} faces | '\n",
    "          f'V={vol_div:.6f} (err={err_vol:.4f}%) | A={area:.6f} (err={err_area:.4f}%)')\n",
    "\n",
    "df_ico = pd.DataFrame(ico_results)\n",
    "print(f'\\nTarget Volume: {V_analytical:.6f}')\n",
    "print(f'Level 5 Error: {df_ico.iloc[-1][\"Vol Error (%)\"]:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Icosphere levels + convergence plot\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=5,\n",
    "    specs=[[{'type':'scene'}]*5],\n",
    "    subplot_titles=[f'Level {i} ({df_ico.iloc[i-1][\"Vertices\"]} verts)' for i in range(1,6)],\n",
    ")\n",
    "for level in range(1, 6):\n",
    "    v, f = create_icosphere(subdivisions=level, radius=1.0)\n",
    "    fig.add_trace(go.Mesh3d(\n",
    "        x=v[:,0], y=v[:,1], z=v[:,2],\n",
    "        i=f[:,0], j=f[:,1], k=f[:,2],\n",
    "        color='steelblue', opacity=0.8, flatshading=True, showscale=False,\n",
    "    ), row=1, col=level)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    sc = 'scene' if i == 1 else f'scene{i}'\n",
    "    fig.update_layout(**{sc: dict(aspectmode='data', camera=dict(eye=dict(x=1.5,y=1.5,z=1.5)))})\n",
    "fig.update_layout(title='Icosphere Subdivision Levels 1-5', width=1500, height=350)\n",
    "fig.show()\n",
    "\n",
    "# Convergence plot\n",
    "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.semilogy(df_ico['Level'], df_ico['Vol Error (%)'], 'bo-', markersize=10, linewidth=2, label='Volume Error')\n",
    "ax1.semilogy(df_ico['Level'], df_ico['Area Error (%)'], 'rs-', markersize=10, linewidth=2, label='Area Error')\n",
    "ax1.set_xlabel('Icosphere Level', fontsize=12)\n",
    "ax1.set_ylabel('Relative Error (%)', fontsize=12)\n",
    "ax1.set_title('Icosphere: Error vs Subdivision Level (박문기 Table 재현)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11); ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(1, 6))\n",
    "\n",
    "ax2.semilogy(df_ico['Vertices'], df_ico['Vol Error (%)'], 'bo-', markersize=10, linewidth=2, label='Volume Error')\n",
    "ax2.semilogy(df_ico['Vertices'], df_ico['Area Error (%)'], 'rs-', markersize=10, linewidth=2, label='Area Error')\n",
    "ax2.set_xlabel('Number of Vertices', fontsize=12)\n",
    "ax2.set_ylabel('Relative Error (%)', fontsize=12)\n",
    "ax2.set_title('Error vs Mesh Resolution', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print table matching reference paper format\n",
    "print('\\n=== Icosphere Precision Table (Reference: 박문기 p.2) ===')\n",
    "print(f'{\"Level\":>5} {\"Vertices\":>10} {\"Faces\":>8} {\"Volume\":>12} {\"Error (%)\":>12} {\"Area\":>12} {\"Area Err(%)\":>12}')\n",
    "print('-' * 80)\n",
    "for _, r in df_ico.iterrows():\n",
    "    print(f'{int(r[\"Level\"]):>5} {int(r[\"Vertices\"]):>10,} {int(r[\"Faces\"]):>8,} '\n",
    "          f'{r[\"Volume (Div)\"]:>12.6f} {r[\"Vol Error (%)\"]:>12.4f} '\n",
    "          f'{r[\"Surface Area\"]:>12.6f} {r[\"Area Error (%)\"]:>12.4f}')\n",
    "print(f'{\"Exact\":>5} {\"-\":>10} {\"-\":>8} {V_analytical:>12.6f} {0.0:>12.4f} {A_analytical:>12.6f} {0.0:>12.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Green's Theorem 2D - Cross-Section Area\n",
    "\n",
    "**Reference: 박문기 (Green's Theorem), 박도현 (Green's Equations per slice)**\n",
    "\n",
    "Green's Theorem converts a double integral over a region to a line integral along its boundary:\n",
    "\n",
    "$$A = \\oint_C x\\,dy = \\frac{1}{2} \\oint_C (x\\,dy - y\\,dx) = \\frac{1}{2}\\sum_{i=0}^{n-1}(x_i y_{i+1} - x_{i+1} y_i)$$\n",
    "\n",
    "This is the **Shoelace formula**, directly derived from Green's theorem.  \n",
    "We apply this to cross-sections of the Stanford Bunny mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_cross_section(verts, faces, axis=1, level=None, tolerance=None):\n",
    "    \"\"\"Extract cross-section contour of a mesh at a given axis level.\n",
    "    Returns ordered 2D points of the cross-section boundary.\"\"\"\n",
    "    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n",
    "    if level is None:\n",
    "        level = verts[:, axis].mean()\n",
    "    \n",
    "    # Use trimesh section\n",
    "    origin = np.zeros(3)\n",
    "    origin[axis] = level\n",
    "    normal = np.zeros(3)\n",
    "    normal[axis] = 1.0\n",
    "    \n",
    "    section = mesh.section(plane_origin=origin, plane_normal=normal)\n",
    "    if section is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Get 2D path\n",
    "    try:\n",
    "        path_2d, transform = section.to_planar()\n",
    "        # Get vertices from the path\n",
    "        if len(path_2d.polygons_closed) > 0:\n",
    "            # Get the largest polygon\n",
    "            polygon = max(path_2d.polygons_closed, key=lambda p: p.area)\n",
    "            coords = np.array(polygon.exterior.coords)\n",
    "            return coords[:, 0], coords[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Demonstrate Green's theorem on unit circle first (validation)\n",
    "print('=== Validation: Green\\'s Theorem on Unit Circle ===')\n",
    "for n_pts in [10, 50, 100, 500, 1000]:\n",
    "    theta = np.linspace(0, 2*np.pi, n_pts, endpoint=False)\n",
    "    cx, cy = np.cos(theta), np.sin(theta)\n",
    "    area = greens_theorem_2d_vectorized(cx, cy)\n",
    "    err = abs(area - np.pi) / np.pi * 100\n",
    "    print(f'  n={n_pts:>5}: Area={area:.8f}, Error={err:.6f}% (exact={np.pi:.8f})')\n",
    "\n",
    "# Cross-sections of Stanford Bunny\n",
    "print('\\n=== Stanford Bunny Cross-Sections (Green\\'s Theorem) ===')\n",
    "y_min, y_max = vertices[:, 1].min(), vertices[:, 1].max()\n",
    "y_range = y_max - y_min\n",
    "n_slices_demo = 10\n",
    "y_levels = np.linspace(y_min + y_range * 0.1, y_max - y_range * 0.1, n_slices_demo)\n",
    "\n",
    "cross_sections = []\n",
    "for y_lev in y_levels:\n",
    "    cx, cy = mesh_cross_section(vertices, faces, axis=1, level=y_lev)\n",
    "    if cx is not None:\n",
    "        area = greens_theorem_2d_vectorized(cx, cy)\n",
    "        cross_sections.append({'y_level': y_lev, 'area': area, 'n_points': len(cx), 'cx': cx, 'cy': cy})\n",
    "        print(f'  y={y_lev:+.5f}: {len(cx):>4} pts, Area={area:.8f}')\n",
    "    else:\n",
    "        print(f'  y={y_lev:+.5f}: No cross-section found')\n",
    "\n",
    "print(f'\\nSuccessful cross-sections: {len(cross_sections)}/{n_slices_demo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-sections\n",
    "n_show = min(len(cross_sections), 6)\n",
    "if n_show > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.ravel()\n",
    "    step = max(1, len(cross_sections) // n_show)\n",
    "    shown = 0\n",
    "    for idx in range(0, len(cross_sections), step):\n",
    "        if shown >= n_show:\n",
    "            break\n",
    "        cs = cross_sections[idx]\n",
    "        ax = axes[shown]\n",
    "        ax.fill(cs['cx'], cs['cy'], alpha=0.3, color='steelblue')\n",
    "        ax.plot(cs['cx'], cs['cy'], 'b-', linewidth=1)\n",
    "        ax.set_title(f\"y={cs['y_level']:.4f}\\nArea={cs['area']:.6f} ({cs['n_points']} pts)\", fontsize=10)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        shown += 1\n",
    "    for i in range(shown, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    plt.suptitle('Stanford Bunny - Cross-Sections (Green\\'s Theorem Area)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No cross-sections available for visualization.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Slice-Based Volume Measurement\n",
    "\n",
    "**Reference: 박도현 p.7 - Smoothing Splines + Green's Equations**\n",
    "\n",
    "Algorithm:\n",
    "1. Slice mesh along Y-axis at regular intervals (1mm equivalent)\n",
    "2. Extract boundary contour at each slice\n",
    "3. Apply **smoothing splines** to smooth the contour\n",
    "4. Compute area of each slice using **Green's theorem (Shoelace formula)**\n",
    "5. Integrate areas using **trapezoidal rule** to get total volume\n",
    "\n",
    "$$V_{\\text{slice}} = \\int_{y_{\\min}}^{y_{\\max}} A(y)\\,dy \\approx \\sum_{i=0}^{n-1} \\frac{A(y_i) + A(y_{i+1})}{2} \\cdot \\Delta y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def smooth_contour(cx, cy, smoothing_factor=0.0, n_resample=200):\n    \"\"\"Smooth a 2D contour using parametric smoothing splines.\n    Reference: smoothing splines on cross-section contours.\"\"\"\n    if not (np.allclose(cx[0], cx[-1]) and np.allclose(cy[0], cy[-1])):\n        cx = np.append(cx, cx[0])\n        cy = np.append(cy, cy[0])\n    try:\n        tck, u = splprep([cx, cy], s=smoothing_factor, per=True, k=3)\n        u_new = np.linspace(0, 1, n_resample)\n        cx_smooth, cy_smooth = splev(u_new, tck)\n        return cx_smooth, cy_smooth\n    except Exception:\n        return cx, cy\n\n\ndef slice_based_volume(verts, faces, axis=1, n_slices=100, smooth=True, smoothing_factor=0.0):\n    \"\"\"Compute volume by slicing mesh and integrating cross-section areas.\n    Reference: 1mm slices + smoothing splines + Green's equations.\"\"\"\n    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n    lo, hi = verts[:, axis].min(), verts[:, axis].max()\n    margin = (hi - lo) * 0.02\n    levels = np.linspace(lo + margin, hi - margin, n_slices)\n    \n    areas = []\n    valid_levels = []\n    \n    for lev in levels:\n        origin = np.zeros(3); origin[axis] = lev\n        normal = np.zeros(3); normal[axis] = 1.0\n        section = mesh.section(plane_origin=origin, plane_normal=normal)\n        if section is None:\n            continue\n        try:\n            path_2d, _ = section.to_planar()\n            if len(path_2d.polygons_closed) > 0:\n                polygon = max(path_2d.polygons_closed, key=lambda p: p.area)\n                coords = np.array(polygon.exterior.coords)\n                cx, cy = coords[:, 0], coords[:, 1]\n                \n                if smooth and len(cx) >= 4:\n                    cx, cy = smooth_contour(cx, cy, smoothing_factor=smoothing_factor)\n                \n                area = greens_theorem_2d_vectorized(cx, cy)\n                areas.append(area)\n                valid_levels.append(lev)\n        except Exception:\n            continue\n    \n    if len(areas) < 2:\n        return 0.0, np.array([]), np.array([])\n    \n    areas = np.array(areas)\n    valid_levels = np.array(valid_levels)\n    \n    # Trapezoidal integration (np.trapezoid for NumPy 2.x)\n    _trapz = getattr(np, 'trapezoid', getattr(np, 'trapz', None))\n    volume = _trapz(areas, valid_levels)\n    return volume, valid_levels, areas\n\n\n# Run slice-based volume with different slice counts and smoothing\nprint('=== Slice-Based Volume Measurement ===')\nprint('(Reference: Smoothing Splines + Green\\'s Equations)\\n')\n\nslice_results = []\nvol_ref = volume_divergence_theorem(vertices, faces)\n\nfor n_sl in [20, 50, 100, 200]:\n    for smooth, s_factor, s_label in [(False, 0.0, 'None'), (True, 0.0, 's=0'), (True, 1e-6, 's=1e-6')]:\n        vol, levels, areas = slice_based_volume(vertices, faces, n_slices=n_sl, \n                                                smooth=smooth, smoothing_factor=s_factor)\n        err = abs(vol - vol_ref) / vol_ref * 100 if vol_ref > 0 else float('inf')\n        slice_results.append({\n            'Slices': n_sl,\n            'Smoothing': s_label,\n            'Volume': vol,\n            'Valid Slices': len(areas),\n            'Err vs DivThm (%)': err,\n        })\n        print(f'  n={n_sl:>3}, smooth={s_label:>6}: V={vol:.8f} ({len(areas)} valid slices, err={err:.2f}%)')\n\ndf_slice = pd.DataFrame(slice_results)\nprint(f'\\nDivergence Theorem reference: {vol_ref:.8f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed visualization for best slice-based result\nvol_best, levels_best, areas_best = slice_based_volume(vertices, faces, n_slices=100, \n                                                        smooth=True, smoothing_factor=0.0)\n\nif len(areas_best) > 1:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Area profile\n    ax1.fill_between(levels_best, areas_best, alpha=0.3, color='steelblue')\n    ax1.plot(levels_best, areas_best, 'b-', linewidth=2)\n    ax1.set_xlabel('Y-axis Position', fontsize=12)\n    ax1.set_ylabel('Cross-Section Area', fontsize=12)\n    ax1.set_title(f'Slice Area Profile (V={vol_best:.6f})', fontsize=13, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n\n    # Cumulative volume\n    dy = np.diff(levels_best)\n    avg_areas = (areas_best[:-1] + areas_best[1:]) / 2\n    cum_vol = np.cumsum(avg_areas * dy)\n    ax2.plot(levels_best[1:], cum_vol, 'r-', linewidth=2)\n    ax2.axhline(y=vol_best, color='gray', linestyle='--', alpha=0.5, label=f'Total={vol_best:.6f}')\n    ax2.set_xlabel('Y-axis Position', fontsize=12)\n    ax2.set_ylabel('Cumulative Volume', fontsize=12)\n    ax2.set_title('Cumulative Volume Along Slicing Axis', fontsize=13, fontweight='bold')\n    ax2.legend(fontsize=11); ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('No valid slices - skipping visualization')\n\n# Show smoothing effect\nprint('\\n=== Smoothing Effect on Slice-Based Volume ===')\ndf_smooth = df_slice.pivot_table(index='Slices', columns='Smoothing', values='Volume')\nprint(df_smooth.to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Truncated Cone Approximation\n",
    "\n",
    "**Reference: 박도현 p.5 - LiDAR volume measurement formula**\n",
    "\n",
    "Approximate volume as a stack of truncated cones (frustums) between adjacent slices:\n",
    "\n",
    "$$V = \\frac{1}{3} \\pi h (R^2 + Rr + r^2)$$\n",
    "\n",
    "where $h$ = slice spacing, $R$ and $r$ are equivalent radii from circumference at each slice:\n",
    "$$r = \\frac{C}{2\\pi} = \\sqrt{\\frac{A}{\\pi}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_truncated_cone(verts, faces, axis=1, n_slices=100):\n",
    "    \"\"\"Volume via truncated cone (frustum) approximation.\n",
    "    Reference: 박도현 p.5 - V = (1/3)*pi*h*(R^2 + R*r + r^2)\"\"\"\n",
    "    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n",
    "    lo, hi = verts[:, axis].min(), verts[:, axis].max()\n",
    "    margin = (hi - lo) * 0.02\n",
    "    levels = np.linspace(lo + margin, hi - margin, n_slices)\n",
    "    \n",
    "    radii = []\n",
    "    valid_levels = []\n",
    "    \n",
    "    for lev in levels:\n",
    "        origin = np.zeros(3); origin[axis] = lev\n",
    "        normal = np.zeros(3); normal[axis] = 1.0\n",
    "        section = mesh.section(plane_origin=origin, plane_normal=normal)\n",
    "        if section is None:\n",
    "            continue\n",
    "        try:\n",
    "            path_2d, _ = section.to_planar()\n",
    "            if len(path_2d.polygons_closed) > 0:\n",
    "                polygon = max(path_2d.polygons_closed, key=lambda p: p.area)\n",
    "                area = polygon.area\n",
    "                # Equivalent radius: r = sqrt(A/pi)\n",
    "                r_eq = np.sqrt(area / np.pi)\n",
    "                radii.append(r_eq)\n",
    "                valid_levels.append(lev)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if len(radii) < 2:\n",
    "        return 0.0, np.array([]), np.array([])\n",
    "    \n",
    "    radii = np.array(radii)\n",
    "    valid_levels = np.array(valid_levels)\n",
    "    \n",
    "    # Compute volume as sum of truncated cones\n",
    "    total_vol = 0.0\n",
    "    for i in range(len(radii) - 1):\n",
    "        h = valid_levels[i+1] - valid_levels[i]\n",
    "        R, r = radii[i], radii[i+1]\n",
    "        vol_cone = (1.0/3.0) * np.pi * h * (R**2 + R*r + r**2)\n",
    "        total_vol += vol_cone\n",
    "    \n",
    "    return total_vol, valid_levels, radii\n",
    "\n",
    "\n",
    "# Validate on unit sphere first\n",
    "print('=== Validation: Truncated Cone on Unit Sphere ===')\n",
    "v_sphere, f_sphere = create_icosphere(subdivisions=4, radius=1.0)\n",
    "for n_sl in [20, 50, 100, 200]:\n",
    "    vol_tc, _, _ = volume_truncated_cone(v_sphere, f_sphere, axis=1, n_slices=n_sl)\n",
    "    err = abs(vol_tc - V_analytical) / V_analytical * 100\n",
    "    print(f'  n={n_sl:>3}: V={vol_tc:.6f} (err={err:.4f}%, exact={V_analytical:.6f})')\n",
    "\n",
    "# Apply to Stanford Bunny\n",
    "print('\\n=== Stanford Bunny: Truncated Cone Approximation ===')\n",
    "tc_results = []\n",
    "for n_sl in [20, 50, 100, 200]:\n",
    "    vol_tc, tc_levels, tc_radii = volume_truncated_cone(vertices, faces, n_slices=n_sl)\n",
    "    err_vs_div = abs(vol_tc - vol_ref) / vol_ref * 100 if vol_ref > 0 else float('inf')\n",
    "    tc_results.append({'Slices': n_sl, 'Volume': vol_tc, 'Valid': len(tc_radii), 'Err vs DivThm (%)': err_vs_div})\n",
    "    print(f'  n={n_sl:>3}: V={vol_tc:.8f} ({len(tc_radii)} valid, err={err_vs_div:.2f}%)')\n",
    "\n",
    "df_tc = pd.DataFrame(tc_results)\n",
    "\n",
    "# Visualize radius profile\n",
    "vol_tc_best, tc_levels_best, tc_radii_best = volume_truncated_cone(vertices, faces, n_slices=100)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(tc_levels_best, tc_radii_best, 'g-o', markersize=3, linewidth=1.5)\n",
    "ax1.set_xlabel('Y-axis Position', fontsize=12)\n",
    "ax1.set_ylabel('Equivalent Radius', fontsize=12)\n",
    "ax1.set_title('Equivalent Radius Profile (r = sqrt(A/pi))', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare slice methods\n",
    "methods = ['Slice (no smooth)', 'Slice (s=0)', 'Slice (s=1e-6)', 'Truncated Cone']\n",
    "# Get best results for each\n",
    "slice_100_ns = df_slice[(df_slice['Slices']==100) & (df_slice['Smoothing']=='None')]['Volume'].values[0]\n",
    "slice_100_s0 = df_slice[(df_slice['Slices']==100) & (df_slice['Smoothing']=='s=0')]['Volume'].values[0]\n",
    "slice_100_s6 = df_slice[(df_slice['Slices']==100) & (df_slice['Smoothing']=='s=1e-6')]['Volume'].values[0]\n",
    "tc_100 = df_tc[df_tc['Slices']==100]['Volume'].values[0]\n",
    "\n",
    "vols_compare = [slice_100_ns, slice_100_s0, slice_100_s6, tc_100]\n",
    "colors_c = ['#2196F3', '#1976D2', '#0D47A1', '#4CAF50']\n",
    "ax2.bar(methods, vols_compare, color=colors_c, edgecolor='black', linewidth=0.5)\n",
    "ax2.axhline(y=vol_ref, color='red', linestyle='--', alpha=0.6, label=f'Div.Thm={vol_ref:.6f}')\n",
    "ax2.set_ylabel('Volume', fontsize=12)\n",
    "ax2.set_title('Slice-Based Methods Comparison (100 slices)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10); ax2.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Planar Capping for Non-Watertight Meshes\n",
    "\n",
    "**Reference: 박문기 p.4 - Capping (DeepSDF/Planar)**\n",
    "\n",
    "The Stanford Bunny mesh is **not watertight** (has boundary edges / holes).  \n",
    "This significantly affects volume accuracy.\n",
    "\n",
    "**Planar Capping** strategy:\n",
    "1. Detect boundary edges (edges belonging to only one face)\n",
    "2. Group boundary edges into loops\n",
    "3. Fill each loop with triangles (ear-clipping or fan triangulation)\n",
    "4. Compare volume before/after capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundary_edges(faces):\n",
    "    \"\"\"Find boundary edges (edges used by only one face).\"\"\"\n",
    "    edge_count = {}\n",
    "    for face in faces:\n",
    "        for i in range(3):\n",
    "            e = tuple(sorted([face[i], face[(i+1) % 3]]))\n",
    "            edge_count[e] = edge_count.get(e, 0) + 1\n",
    "    boundary = [e for e, c in edge_count.items() if c == 1]\n",
    "    return boundary\n",
    "\n",
    "\n",
    "def order_boundary_loop(boundary_edges):\n",
    "    \"\"\"Order boundary edges into connected loops.\"\"\"\n",
    "    if not boundary_edges:\n",
    "        return []\n",
    "    \n",
    "    adj = {}\n",
    "    for e in boundary_edges:\n",
    "        adj.setdefault(e[0], []).append(e[1])\n",
    "        adj.setdefault(e[1], []).append(e[0])\n",
    "    \n",
    "    visited_edges = set()\n",
    "    loops = []\n",
    "    \n",
    "    for start_edge in boundary_edges:\n",
    "        start = start_edge[0]\n",
    "        if start in visited_edges:\n",
    "            continue\n",
    "        \n",
    "        loop = [start]\n",
    "        visited_edges.add(start)\n",
    "        current = start\n",
    "        \n",
    "        while True:\n",
    "            found_next = False\n",
    "            for neighbor in adj.get(current, []):\n",
    "                if neighbor not in visited_edges:\n",
    "                    loop.append(neighbor)\n",
    "                    visited_edges.add(neighbor)\n",
    "                    current = neighbor\n",
    "                    found_next = True\n",
    "                    break\n",
    "            if not found_next:\n",
    "                break\n",
    "        \n",
    "        if len(loop) >= 3:\n",
    "            loops.append(loop)\n",
    "    \n",
    "    return loops\n",
    "\n",
    "\n",
    "def cap_mesh(verts, faces):\n",
    "    \"\"\"Cap holes in mesh using fan triangulation from boundary loop centroid.\"\"\"\n",
    "    boundary_edges = find_boundary_edges(faces)\n",
    "    if not boundary_edges:\n",
    "        return verts, faces, 0\n",
    "    \n",
    "    loops = order_boundary_loop(boundary_edges)\n",
    "    new_verts = list(verts)\n",
    "    new_faces = list(faces)\n",
    "    n_cap_faces = 0\n",
    "    \n",
    "    for loop in loops:\n",
    "        if len(loop) < 3:\n",
    "            continue\n",
    "        # Add centroid vertex\n",
    "        centroid = verts[loop].mean(axis=0)\n",
    "        centroid_idx = len(new_verts)\n",
    "        new_verts.append(centroid)\n",
    "        \n",
    "        # Fan triangulation from centroid\n",
    "        for i in range(len(loop)):\n",
    "            j = (i + 1) % len(loop)\n",
    "            new_faces.append([loop[i], loop[j], centroid_idx])\n",
    "            n_cap_faces += 1\n",
    "    \n",
    "    return np.array(new_verts), np.array(new_faces), n_cap_faces\n",
    "\n",
    "\n",
    "# Analyze boundary of all bunny meshes\n",
    "print('=== Boundary Analysis ===')\n",
    "for label, data in meshes.items():\n",
    "    v, f = data['vertices'], data['faces']\n",
    "    boundary = find_boundary_edges(f)\n",
    "    loops = order_boundary_loop(boundary)\n",
    "    mesh_tm = trimesh.Trimesh(vertices=v, faces=f)\n",
    "    print(f'{label:16s}: {len(boundary):>4} boundary edges, '\n",
    "          f'{len(loops)} loops, watertight={mesh_tm.is_watertight}')\n",
    "\n",
    "# Cap the full mesh\n",
    "print('\\n=== Capping Stanford Bunny (Full 35k) ===')\n",
    "v_capped, f_capped, n_cap = cap_mesh(vertices, faces)\n",
    "print(f'Original : {len(vertices):,} verts, {len(faces):,} faces')\n",
    "print(f'Capped   : {len(v_capped):,} verts, {len(f_capped):,} faces (+{n_cap} cap faces)')\n",
    "\n",
    "# Compare volumes\n",
    "mesh_orig = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
    "mesh_capped = trimesh.Trimesh(vertices=v_capped, faces=f_capped)\n",
    "\n",
    "vol_orig_div = volume_divergence_theorem(vertices, faces)\n",
    "vol_orig_tet = volume_signed_tetrahedra(vertices, faces)\n",
    "vol_capped_div = volume_divergence_theorem(v_capped, f_capped)\n",
    "vol_capped_tet = volume_signed_tetrahedra(v_capped, f_capped)\n",
    "\n",
    "print(f'\\n{\"Method\":>25} {\"Original\":>14} {\"Capped\":>14} {\"Change\":>10}')\n",
    "print('-' * 65)\n",
    "print(f'{\"Divergence Theorem\":>25} {vol_orig_div:>14.8f} {vol_capped_div:>14.8f} '\n",
    "      f'{(vol_capped_div/vol_orig_div - 1)*100:>+9.2f}%')\n",
    "print(f'{\"Signed Tetrahedra\":>25} {vol_orig_tet:>14.8f} {vol_capped_tet:>14.8f} '\n",
    "      f'{(vol_capped_tet/vol_orig_tet - 1)*100:>+9.2f}%')\n",
    "print(f'{\"Trimesh\":>25} {abs(mesh_orig.volume):>14.8f} {abs(mesh_capped.volume):>14.8f} '\n",
    "      f'{(abs(mesh_capped.volume)/abs(mesh_orig.volume) - 1)*100:>+9.2f}%')\n",
    "print(f'{\"Watertight?\":>25} {str(mesh_orig.is_watertight):>14} {str(mesh_capped.is_watertight):>14}')\n",
    "\n",
    "# Also try trimesh's built-in fill_holes\n",
    "mesh_filled = mesh_orig.copy()\n",
    "trimesh.repair.fill_holes(mesh_filled)\n",
    "trimesh.repair.fix_winding(mesh_filled)\n",
    "vol_filled = abs(mesh_filled.volume)\n",
    "print(f'{\"Trimesh fill_holes\":>25} {\"-\":>14} {vol_filled:>14.8f} '\n",
    "      f'{(vol_filled/abs(mesh_orig.volume) - 1)*100:>+9.2f}%')\n",
    "print(f'{\"  Watertight?\":>25} {\"-\":>14} {str(mesh_filled.is_watertight):>14}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize boundary edges and capping\n",
    "boundary_edges = find_boundary_edges(faces)\n",
    "boundary_verts = list(set([v for e in boundary_edges for v in e]))\n",
    "bv = vertices[boundary_verts]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Mesh3d(\n",
    "    x=vertices[:,0], y=vertices[:,2], z=vertices[:,1],\n",
    "    i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "    opacity=0.3, color='lightblue', flatshading=True, name='Mesh',\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=bv[:,0], y=bv[:,2], z=bv[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, color='red'),\n",
    "    name=f'Boundary ({len(boundary_verts)} verts)',\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f'Non-Watertight Mesh: Boundary Edges ({len(boundary_edges)} edges)',\n",
    "    scene=dict(aspectmode='data'),\n",
    "    width=900, height=700,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 6: Mesh Resolution Convergence Study\n",
    "\n",
    "**Reference: 박문기 p.2,4 - Point count vs error rate**\n",
    "\n",
    "Two studies:\n",
    "1. **Icosphere convergence**: Analytical ground truth available (sphere)\n",
    "2. **Bunny resolution convergence**: Compare 4 resolution levels (Res4 → Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunny resolution convergence\n",
    "print('=== Bunny Mesh Resolution Convergence ===')\n",
    "print(f'{\"Resolution\":>16} {\"Vertices\":>10} {\"Faces\":>8} {\"DivThm Vol\":>14} {\"Tet Vol\":>14} {\"Area\":>12} {\"WT\":>5}')\n",
    "print('-' * 80)\n",
    "\n",
    "res_data = []\n",
    "for label in ['Res4 (453)', 'Res3 (1.9k)', 'Res2 (8k)', 'Full (35k)']:\n",
    "    v, f = meshes[label]['vertices'], meshes[label]['faces']\n",
    "    vd = volume_divergence_theorem(v, f)\n",
    "    vt = volume_signed_tetrahedra(v, f)\n",
    "    mt = trimesh.Trimesh(vertices=v, faces=f)\n",
    "    res_data.append({\n",
    "        'Label': label, 'Vertices': len(v), 'Faces': len(f),\n",
    "        'Vol_Div': vd, 'Vol_Tet': vt, 'Area': mt.area,\n",
    "        'Watertight': mt.is_watertight, 'Euler': mt.euler_number,\n",
    "    })\n",
    "    print(f'{label:>16} {len(v):>10,} {len(f):>8,} {vd:>14.8f} {vt:>14.8f} {mt.area:>12.8f} {str(mt.is_watertight):>5}')\n",
    "\n",
    "df_res = pd.DataFrame(res_data)\n",
    "\n",
    "# Use Full mesh as reference for error calculation\n",
    "ref_vol = df_res.iloc[-1]['Vol_Div']\n",
    "df_res['Err_vs_Full (%)'] = abs(df_res['Vol_Div'] - ref_vol) / ref_vol * 100\n",
    "\n",
    "print(f'\\nReference (Full): {ref_vol:.8f}')\n",
    "for _, r in df_res.iterrows():\n",
    "    print(f'  {r[\"Label\"]:>16}: {r[\"Err_vs_Full (%)\"]:.4f}% error')\n",
    "\n",
    "# Also cap each resolution and compare\n",
    "print('\\n=== After Capping ===')\n",
    "capped_data = []\n",
    "for label in ['Res4 (453)', 'Res3 (1.9k)', 'Res2 (8k)', 'Full (35k)']:\n",
    "    v, f = meshes[label]['vertices'], meshes[label]['faces']\n",
    "    vc, fc, nc = cap_mesh(v, f)\n",
    "    vd = volume_divergence_theorem(vc, fc)\n",
    "    vt = volume_signed_tetrahedra(vc, fc)\n",
    "    mt = trimesh.Trimesh(vertices=vc, faces=fc)\n",
    "    capped_data.append({\n",
    "        'Label': label, 'Cap Faces': nc,\n",
    "        'Vol_Div': vd, 'Vol_Tet': vt, 'Watertight': mt.is_watertight,\n",
    "    })\n",
    "    print(f'  {label:>16} +{nc:>3} caps: DivThm={vd:.8f}, Tet={vt:.8f}, WT={mt.is_watertight}')\n",
    "\n",
    "df_capped = pd.DataFrame(capped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Icosphere convergence (log-log)\n",
    "ax = axes[0]\n",
    "ax.loglog(df_ico['Vertices'], df_ico['Vol Error (%)'], 'bo-', markersize=10, linewidth=2, label='Volume')\n",
    "ax.loglog(df_ico['Vertices'], df_ico['Area Error (%)'], 'rs-', markersize=10, linewidth=2, label='Area')\n",
    "# Fit power law\n",
    "log_v = np.log(df_ico['Vertices'].values)\n",
    "log_e = np.log(df_ico['Vol Error (%)'].values)\n",
    "slope, intercept = np.polyfit(log_v, log_e, 1)\n",
    "ax.set_xlabel('Vertices (log)', fontsize=12)\n",
    "ax.set_ylabel('Error % (log)', fontsize=12)\n",
    "ax.set_title(f'Icosphere Convergence\\n(slope={slope:.2f})', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# 2. Bunny resolution convergence\n",
    "ax = axes[1]\n",
    "ax.plot(df_res['Vertices'], df_res['Vol_Div'], 'bo-', markersize=10, linewidth=2, label='Div. Theorem')\n",
    "ax.plot(df_res['Vertices'], df_res['Vol_Tet'], 'rs--', markersize=10, linewidth=2, label='Signed Tet.')\n",
    "ax.set_xlabel('Vertices', fontsize=12)\n",
    "ax.set_ylabel('Volume', fontsize=12)\n",
    "ax.set_title('Bunny: Volume vs Resolution', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Bunny before/after capping\n",
    "ax = axes[2]\n",
    "x_pos = np.arange(len(df_res))\n",
    "w = 0.35\n",
    "ax.bar(x_pos - w/2, df_res['Vol_Div'], w, color='steelblue', label='Original', edgecolor='black', linewidth=0.5)\n",
    "ax.bar(x_pos + w/2, df_capped['Vol_Div'], w, color='coral', label='Capped', edgecolor='black', linewidth=0.5)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([l.split()[0] for l in df_res['Label']], fontsize=9)\n",
    "ax.set_ylabel('Volume (Div. Theorem)', fontsize=12)\n",
    "ax.set_title('Volume Before/After Capping', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 7: Bland-Altman Analysis & Statistical Metrics\n",
    "\n",
    "**Reference: 박도현 p.5,7,8**\n",
    "\n",
    "### Bland-Altman Analysis\n",
    "Compare agreement between measurement methods:\n",
    "- Mean difference (bias)\n",
    "- 95% limits of agreement (mean diff +/- 1.96*SD)\n",
    "\n",
    "### Statistical Metrics\n",
    "- **TEM** (Technical Error of Measurement): $\\text{TEM} = \\sqrt{\\frac{\\sum d_i^2}{2n}}$\n",
    "- **CV** (Coefficient of Variation): $\\text{CV} = \\frac{\\text{SD}}{\\bar{x}} \\times 100$\n",
    "- **ICC** (Intraclass Correlation Coefficient): consistency across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all volume measurements across resolutions and methods\n",
    "print('=== Collecting Volume Measurements Across Resolutions ===')\n",
    "\n",
    "all_measurements = []\n",
    "for label in ['Res4 (453)', 'Res3 (1.9k)', 'Res2 (8k)', 'Full (35k)']:\n",
    "    v, f = meshes[label]['vertices'], meshes[label]['faces']\n",
    "    vc, fc, _ = cap_mesh(v, f)\n",
    "    \n",
    "    methods = {\n",
    "        'Div.Theorem': volume_divergence_theorem(v, f),\n",
    "        'Signed Tet.': volume_signed_tetrahedra(v, f),\n",
    "        'Trimesh': abs(trimesh.Trimesh(vertices=v, faces=f).volume),\n",
    "        'Div.Thm (capped)': volume_divergence_theorem(vc, fc),\n",
    "        'Tet (capped)': volume_signed_tetrahedra(vc, fc),\n",
    "    }\n",
    "    \n",
    "    # Slice-based (only for meshes with enough resolution)\n",
    "    if len(v) >= 1000:\n",
    "        vol_sl, _, _ = slice_based_volume(v, f, n_slices=80, smooth=True)\n",
    "        methods['Slice-Based'] = vol_sl\n",
    "        vol_tc_m, _, _ = volume_truncated_cone(v, f, n_slices=80)\n",
    "        methods['Trunc. Cone'] = vol_tc_m\n",
    "    \n",
    "    for method, vol in methods.items():\n",
    "        all_measurements.append({'Resolution': label, 'Method': method, 'Volume': vol})\n",
    "\n",
    "df_all = pd.DataFrame(all_measurements)\n",
    "\n",
    "# Pivot table\n",
    "pivot = df_all.pivot_table(index='Resolution', columns='Method', values='Volume')\n",
    "print(pivot.to_string(float_format='{:.8f}'.format))\n",
    "\n",
    "\n",
    "def bland_altman(m1, m2):\n",
    "    \"\"\"Bland-Altman analysis: returns mean, diff, mean_diff, std_diff, LoA.\"\"\"\n",
    "    m1, m2 = np.asarray(m1), np.asarray(m2)\n",
    "    mean_vals = (m1 + m2) / 2\n",
    "    diffs = m1 - m2\n",
    "    mean_diff = np.mean(diffs)\n",
    "    std_diff = np.std(diffs, ddof=1)\n",
    "    loa_upper = mean_diff + 1.96 * std_diff\n",
    "    loa_lower = mean_diff - 1.96 * std_diff\n",
    "    return mean_vals, diffs, mean_diff, std_diff, loa_upper, loa_lower\n",
    "\n",
    "\n",
    "def compute_tem(diffs):\n",
    "    \"\"\"Technical Error of Measurement: TEM = sqrt(sum(d^2) / (2*n))\"\"\"\n",
    "    diffs = np.asarray(diffs)\n",
    "    return np.sqrt(np.sum(diffs**2) / (2 * len(diffs)))\n",
    "\n",
    "\n",
    "def compute_cv(values):\n",
    "    \"\"\"Coefficient of Variation: CV = SD/mean * 100\"\"\"\n",
    "    values = np.asarray(values)\n",
    "    return np.std(values, ddof=1) / np.mean(values) * 100 if np.mean(values) != 0 else 0\n",
    "\n",
    "\n",
    "def compute_icc(data_matrix):\n",
    "    \"\"\"ICC(3,1) - Two-way mixed, single measures, consistency.\n",
    "    data_matrix: rows=subjects (resolutions), columns=methods\"\"\"\n",
    "    n, k = data_matrix.shape\n",
    "    grand_mean = data_matrix.mean()\n",
    "    row_means = data_matrix.mean(axis=1)\n",
    "    col_means = data_matrix.mean(axis=0)\n",
    "    \n",
    "    ss_total = np.sum((data_matrix - grand_mean)**2)\n",
    "    ss_rows = k * np.sum((row_means - grand_mean)**2)\n",
    "    ss_cols = n * np.sum((col_means - grand_mean)**2)\n",
    "    ss_error = ss_total - ss_rows - ss_cols\n",
    "    \n",
    "    ms_rows = ss_rows / (n - 1)\n",
    "    ms_error = ss_error / ((n - 1) * (k - 1))\n",
    "    \n",
    "    icc = (ms_rows - ms_error) / (ms_rows + (k - 1) * ms_error) if (ms_rows + (k - 1) * ms_error) != 0 else 0\n",
    "    return icc\n",
    "\n",
    "print('\\nStatistical functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bland-Altman plots for key method pairs\n",
    "# Use Full mesh measurements across different methods\n",
    "full_only = df_all[df_all['Resolution'] == 'Full (35k)'].set_index('Method')['Volume']\n",
    "\n",
    "# Get measurements across resolutions for paired comparisons\n",
    "common_methods = ['Div.Theorem', 'Signed Tet.', 'Trimesh']\n",
    "res_labels = ['Res4 (453)', 'Res3 (1.9k)', 'Res2 (8k)', 'Full (35k)']\n",
    "\n",
    "pairs = [\n",
    "    ('Div.Theorem', 'Signed Tet.'),\n",
    "    ('Div.Theorem', 'Trimesh'),\n",
    "    ('Div.Theorem', 'Div.Thm (capped)'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (m1_name, m2_name) in enumerate(pairs):\n",
    "    m1_vals, m2_vals = [], []\n",
    "    for res in res_labels:\n",
    "        subset = df_all[df_all['Resolution'] == res].set_index('Method')['Volume']\n",
    "        if m1_name in subset.index and m2_name in subset.index:\n",
    "            m1_vals.append(subset[m1_name])\n",
    "            m2_vals.append(subset[m2_name])\n",
    "    \n",
    "    if len(m1_vals) < 2:\n",
    "        axes[idx].text(0.5, 0.5, 'Insufficient data', ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        continue\n",
    "    \n",
    "    means, diffs, md, sd, upper, lower = bland_altman(m1_vals, m2_vals)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(means, diffs, s=80, color='steelblue', edgecolors='black', zorder=3)\n",
    "    ax.axhline(md, color='red', linestyle='-', linewidth=1.5, label=f'Mean={md:.2e}')\n",
    "    ax.axhline(upper, color='gray', linestyle='--', linewidth=1, label=f'+1.96SD={upper:.2e}')\n",
    "    ax.axhline(lower, color='gray', linestyle='--', linewidth=1, label=f'-1.96SD={lower:.2e}')\n",
    "    ax.fill_between(ax.get_xlim(), lower, upper, alpha=0.1, color='gray')\n",
    "    ax.set_xlabel('Mean of Two Methods', fontsize=11)\n",
    "    ax.set_ylabel('Difference', fontsize=11)\n",
    "    ax.set_title(f'{m1_name} vs {m2_name}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bland-Altman Analysis (Reference: 박도현 p.5)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Metrics: TEM, CV, ICC\n",
    "print('=== Statistical Metrics (Reference: 박도현 p.7-8) ===\\n')\n",
    "\n",
    "# 1. TEM for each method pair\n",
    "print('--- TEM (Technical Error of Measurement) ---')\n",
    "print(f'{\"Method Pair\":>40} {\"TEM\":>14} {\"rTEM (%)\":>10}')\n",
    "print('-' * 66)\n",
    "for m1_name, m2_name in pairs:\n",
    "    m1_vals, m2_vals = [], []\n",
    "    for res in res_labels:\n",
    "        subset = df_all[df_all['Resolution'] == res].set_index('Method')['Volume']\n",
    "        if m1_name in subset.index and m2_name in subset.index:\n",
    "            m1_vals.append(subset[m1_name])\n",
    "            m2_vals.append(subset[m2_name])\n",
    "    if len(m1_vals) >= 2:\n",
    "        diffs = np.array(m1_vals) - np.array(m2_vals)\n",
    "        tem = compute_tem(diffs)\n",
    "        mean_val = np.mean(m1_vals + m2_vals)\n",
    "        rtem = tem / mean_val * 100 if mean_val != 0 else 0\n",
    "        print(f'{m1_name + \" vs \" + m2_name:>40} {tem:>14.2e} {rtem:>9.4f}%')\n",
    "\n",
    "# 2. CV for each method across resolutions\n",
    "print(f'\\n--- CV (Coefficient of Variation) ---')\n",
    "print(f'{\"Method\":>20} {\"CV (%)\":>10}')\n",
    "print('-' * 32)\n",
    "for method in common_methods + ['Div.Thm (capped)', 'Tet (capped)']:\n",
    "    vals = df_all[df_all['Method'] == method]['Volume'].values\n",
    "    if len(vals) >= 2:\n",
    "        cv = compute_cv(vals)\n",
    "        print(f'{method:>20} {cv:>9.4f}%')\n",
    "\n",
    "# 3. ICC across methods\n",
    "print(f'\\n--- ICC (Intraclass Correlation Coefficient) ---')\n",
    "# Build matrix: rows = resolutions, columns = methods (use common methods only)\n",
    "icc_methods = ['Div.Theorem', 'Signed Tet.', 'Trimesh']\n",
    "icc_matrix = []\n",
    "for res in res_labels:\n",
    "    row = []\n",
    "    subset = df_all[df_all['Resolution'] == res].set_index('Method')['Volume']\n",
    "    for m in icc_methods:\n",
    "        if m in subset.index:\n",
    "            row.append(subset[m])\n",
    "    if len(row) == len(icc_methods):\n",
    "        icc_matrix.append(row)\n",
    "\n",
    "if len(icc_matrix) >= 2:\n",
    "    icc_matrix = np.array(icc_matrix)\n",
    "    icc_val = compute_icc(icc_matrix)\n",
    "    print(f'  ICC(3,1) for {icc_methods}: {icc_val:.6f}')\n",
    "    print(f'  Interpretation: {\"Excellent\" if icc_val > 0.9 else \"Good\" if icc_val > 0.75 else \"Moderate\" if icc_val > 0.5 else \"Poor\"}')\n",
    "\n",
    "# ICC for capped methods\n",
    "icc_methods_c = ['Div.Thm (capped)', 'Tet (capped)']\n",
    "icc_matrix_c = []\n",
    "for res in res_labels:\n",
    "    row = []\n",
    "    subset = df_all[df_all['Resolution'] == res].set_index('Method')['Volume']\n",
    "    for m in icc_methods_c:\n",
    "        if m in subset.index:\n",
    "            row.append(subset[m])\n",
    "    if len(row) == len(icc_methods_c):\n",
    "        icc_matrix_c.append(row)\n",
    "\n",
    "if len(icc_matrix_c) >= 2:\n",
    "    icc_matrix_c = np.array(icc_matrix_c)\n",
    "    icc_val_c = compute_icc(icc_matrix_c)\n",
    "    print(f'  ICC(3,1) for {icc_methods_c}: {icc_val_c:.6f}')\n",
    "    print(f'  Interpretation: {\"Excellent\" if icc_val_c > 0.9 else \"Good\" if icc_val_c > 0.75 else \"Moderate\" if icc_val_c > 0.5 else \"Poor\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 8: Comprehensive Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison on Full mesh\n",
    "print('=' * 90)\n",
    "print('  COMPREHENSIVE VOLUME MEASUREMENT COMPARISON')\n",
    "print('  Stanford Bunny - Full Resolution (35k vertices, 69k faces)')\n",
    "print('=' * 90)\n",
    "\n",
    "# Gather all methods\n",
    "vol_slice, _, _ = slice_based_volume(vertices, faces, n_slices=100, smooth=True)\n",
    "vol_tc, _, _ = volume_truncated_cone(vertices, faces, n_slices=100)\n",
    "\n",
    "final_results = pd.DataFrame([\n",
    "    {'Method': 'Divergence Theorem', 'Volume': vol_ref, 'Category': 'Surface Integral', 'Ref': '박문기'},\n",
    "    {'Method': 'Signed Tetrahedra', 'Volume': volume_signed_tetrahedra(vertices, faces), 'Category': 'Geometric', 'Ref': '-'},\n",
    "    {'Method': 'Trimesh (library)', 'Volume': abs(trimesh.Trimesh(vertices=vertices, faces=faces).volume), 'Category': 'Library', 'Ref': '-'},\n",
    "    {'Method': 'Div.Thm (capped)', 'Volume': vol_capped_div, 'Category': 'Surface + Capping', 'Ref': '박문기 p.4'},\n",
    "    {'Method': 'Tet (capped)', 'Volume': vol_capped_tet, 'Category': 'Geometric + Capping', 'Ref': '박문기 p.4'},\n",
    "    {'Method': 'Slice-Based (100)', 'Volume': vol_slice, 'Category': 'Cross-Section', 'Ref': '박도현 p.7'},\n",
    "    {'Method': 'Truncated Cone (100)', 'Volume': vol_tc, 'Category': 'Frustum Approx.', 'Ref': '박도현 p.5'},\n",
    "    {'Method': 'Convex Hull', 'Volume': ConvexHull(vertices).volume, 'Category': 'Upper Bound', 'Ref': '-'},\n",
    "])\n",
    "\n",
    "# Use signed tetrahedra as reference (most robust for non-watertight)\n",
    "ref_tet = volume_signed_tetrahedra(vertices, faces)\n",
    "final_results['vs Tet (%)'] = abs(final_results['Volume'] - ref_tet) / ref_tet * 100\n",
    "\n",
    "print(final_results.to_string(index=False, float_format='{:.8f}'.format))\n",
    "print('=' * 90)\n",
    "\n",
    "# Key findings\n",
    "print('\\n=== Key Findings ===')\n",
    "print(f'1. Mesh is NOT watertight -> Divergence Theorem overestimates ({vol_ref:.6f} vs {ref_tet:.6f})')\n",
    "print(f'2. Signed Tetrahedra and Trimesh agree closely (diff={abs(ref_tet - abs(trimesh.Trimesh(vertices=vertices, faces=faces).volume)):.2e})')\n",
    "print(f'3. Capping changes volume by {abs(vol_capped_div/vol_ref - 1)*100:.2f}% (Div.Thm) / {abs(vol_capped_tet/ref_tet - 1)*100:.2f}% (Tet)')\n",
    "print(f'4. Slice-based volume: {vol_slice:.6f} ({abs(vol_slice - ref_tet)/ref_tet*100:.2f}% vs Tet)')\n",
    "print(f'5. Truncated cone: {vol_tc:.6f} ({abs(vol_tc - ref_tet)/ref_tet*100:.2f}% vs Tet)')\n",
    "print(f'6. Icosphere Level 5 error: {df_ico.iloc[-1][\"Vol Error (%)\"]:.4f}% (on watertight mesh)')\n",
    "hull_ratio = ConvexHull(vertices).volume / ref_tet\n",
    "print(f'7. Convex Hull / Tet ratio: {hull_ratio:.2f}x (concavity indicator)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. All methods comparison bar chart\n",
    "ax = axes[0, 0]\n",
    "colors_final = ['#2196F3', '#1976D2', '#0D47A1', '#FF5722', '#E64A19', '#4CAF50', '#388E3C', '#F44336']\n",
    "bars = ax.barh(final_results['Method'], final_results['Volume'], color=colors_final, edgecolor='black', linewidth=0.5)\n",
    "ax.axvline(x=ref_tet, color='red', linestyle='--', alpha=0.6, label=f'Tet ref={ref_tet:.6f}')\n",
    "ax.set_xlabel('Volume', fontsize=12)\n",
    "ax.set_title('All Volume Methods Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Icosphere convergence\n",
    "ax = axes[0, 1]\n",
    "ax.semilogy(df_ico['Level'], df_ico['Vol Error (%)'], 'bo-', markersize=10, linewidth=2)\n",
    "for _, r in df_ico.iterrows():\n",
    "    ax.annotate(f'{r[\"Vol Error (%)\"]:.3f}%', (r['Level'], r['Vol Error (%)']),\n",
    "                textcoords='offset points', xytext=(10, 5), fontsize=9)\n",
    "ax.set_xlabel('Icosphere Level', fontsize=12)\n",
    "ax.set_ylabel('Volume Error (%)', fontsize=12)\n",
    "ax.set_title('Icosphere Precision (박문기 Table)', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(1, 6)); ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# 3. Slice count convergence\n",
    "ax = axes[1, 0]\n",
    "for smooth_label, marker, color in [('None', 'o', 'blue'), ('s=0', 's', 'green'), ('s=1e-6', '^', 'red')]:\n",
    "    subset = df_slice[df_slice['Smoothing'] == smooth_label]\n",
    "    ax.plot(subset['Slices'], subset['Volume'], f'{marker}-', color=color, \n",
    "            markersize=8, linewidth=1.5, label=f'Smooth={smooth_label}')\n",
    "ax.plot(df_tc['Slices'], df_tc['Volume'], 'D--', color='purple', markersize=8, linewidth=1.5, label='Trunc. Cone')\n",
    "ax.axhline(y=ref_tet, color='red', linestyle=':', alpha=0.5, label=f'Tet ref')\n",
    "ax.set_xlabel('Number of Slices', fontsize=12)\n",
    "ax.set_ylabel('Volume', fontsize=12)\n",
    "ax.set_title('Slice Count Convergence (박도현 Approach)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='best'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Method error heatmap\n",
    "ax = axes[1, 1]\n",
    "# Create method x resolution error matrix\n",
    "heat_methods = ['Div.Theorem', 'Signed Tet.', 'Trimesh', 'Div.Thm (capped)', 'Tet (capped)']\n",
    "heat_data = []\n",
    "for res in res_labels:\n",
    "    row = []\n",
    "    subset = df_all[df_all['Resolution'] == res].set_index('Method')['Volume']\n",
    "    for m in heat_methods:\n",
    "        if m in subset.index:\n",
    "            row.append(subset[m])\n",
    "        else:\n",
    "            row.append(np.nan)\n",
    "    heat_data.append(row)\n",
    "heat_matrix = np.array(heat_data)\n",
    "# Normalize by row mean\n",
    "row_means = np.nanmean(heat_matrix, axis=1, keepdims=True)\n",
    "heat_pct = (heat_matrix / row_means - 1) * 100\n",
    "\n",
    "im = ax.imshow(heat_pct, cmap='RdBu_r', aspect='auto', vmin=-50, vmax=50)\n",
    "ax.set_xticks(range(len(heat_methods)))\n",
    "ax.set_xticklabels([m.replace(' ', '\\n') for m in heat_methods], fontsize=8)\n",
    "ax.set_yticks(range(len(res_labels)))\n",
    "ax.set_yticklabels([r.split()[0] for r in res_labels], fontsize=10)\n",
    "for i in range(len(res_labels)):\n",
    "    for j in range(len(heat_methods)):\n",
    "        if not np.isnan(heat_pct[i, j]):\n",
    "            ax.text(j, i, f'{heat_pct[i,j]:+.1f}%', ha='center', va='center', fontsize=8)\n",
    "ax.set_title('Method Deviation from Row Mean (%)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save all experiment results\nexperiment_output = {\n    'experiment': 'Reference-Based Volumetric Measurements',\n    'references': ['박문기 - 체적 측정', '박도현 - DIMA 69차'],\n    'dataset': 'Stanford Bunny',\n    'icosphere_convergence': {\n        'target_volume': V_analytical,\n        'results': df_ico[['Level', 'Vertices', 'Faces', 'Volume (Div)', 'Vol Error (%)']].to_dict('records'),\n    },\n    'bunny_volumes': {\n        'divergence_theorem': float(vol_ref),\n        'signed_tetrahedra': float(ref_tet),\n        'trimesh': float(abs(trimesh.Trimesh(vertices=vertices, faces=faces).volume)),\n        'capped_div': float(vol_capped_div),\n        'capped_tet': float(vol_capped_tet),\n        'slice_based_100': float(vol_slice),\n        'truncated_cone_100': float(vol_tc),\n        'convex_hull': float(ConvexHull(vertices).volume),\n    },\n    'capping': {\n        'boundary_edges': len(boundary_edges),\n        'cap_faces_added': int(n_cap),\n    },\n    'slice_convergence': df_slice.to_dict('records'),\n    'truncated_cone_convergence': df_tc.to_dict('records'),\n    'resolution_study': df_res[['Label', 'Vertices', 'Faces', 'Vol_Div', 'Vol_Tet', 'Area', 'Err_vs_Full (%)']].to_dict('records'),\n}\n\n# Save locally\nlocal_path = os.path.join(WORK_DIR, 'experiment_results.json')\nwith open(local_path, 'w') as f:\n    json.dump(experiment_output, f, indent=2, default=str)\n\n# Copy to Drive if in Colab\nif IN_COLAB and PROJECT_DIR != WORK_DIR:\n    drive_output = os.path.join(PROJECT_DIR, 'experiment_results.json')\n    shutil.copy(local_path, drive_output)\n    print(f'Results saved to Drive: {drive_output}')\nelse:\n    print(f'Results saved to: {local_path}')\n\nprint()\nprint('=' * 70)\nprint('  ALL EXPERIMENTS COMPLETE')\nprint('=' * 70)\nprint(f'  Exp 1: Icosphere L1-5 convergence (12.65% -> {df_ico.iloc[-1][\"Vol Error (%)\"]:.3f}%)')\nprint(f'  Exp 2: Green\\'s Theorem cross-sections ({len(cross_sections)} slices computed)')\nprint(f'  Exp 3: Slice-based volume = {vol_slice:.8f}')\nprint(f'  Exp 4: Truncated cone volume = {vol_tc:.8f}')\nprint(f'  Exp 5: Capping added {n_cap} faces, WT={mesh_capped.is_watertight}')\nprint(f'  Exp 6: Resolution convergence (4 levels studied)')\nprint(f'  Exp 7: Bland-Altman + TEM/CV/ICC analysis')\nprint(f'  Exp 8: Comprehensive comparison (8 methods)')\nprint('=' * 70)"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}